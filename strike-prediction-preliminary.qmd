---
title: "Strike Activity Prediction - Preliminary Analysis"
format: html
---

## Setup

```{r}
#| label: setup
#| message: false

library(tidymodels)
library(dplyr)
library(lubridate)
library(arrow)
library(stringr)
library(zoo)
library(ggplot2)
library(corrplot)
library(gridExtra)
library(vip)
library(conflicted)

# Set conflict preferences
conflicts_prefer(dplyr::lag)

# Set tidymodels preferences
tidymodels_prefer()
```

## Load and Prepare Data

```{r}
#| label: load-data

# Load the GDELT strikes data
all_strikes_data <- read_parquet("data/raw/gdelt_strikes.parquet")

# Check data structure
glimpse(all_strikes_data)
```

## Data Wrangling and Feature Engineering 

***Query: Is this expanded to all weeks and regions?***

```{r}
#| label: data-wrangling

# Data wrangling and feature engineering
weekly_strikes <- all_strikes_data |>
  # Keep only strikes with genuine ADM1 codes (regional detail)
  filter(
    !is.na(ActionGeo_ADM1Code), 
    !is.na(ActionGeo_CountryCode),
    ActionGeo_ADM1Code != ActionGeo_CountryCode  # Filter out country-only codes
  ) |>
mutate(
  date = as.Date(SQLDATE, format = "%Y%m%d"),
  year_week = floor_date(date, "week"),
  # Convert character columns to numeric
  NumMentions = as.numeric(NumMentions),
  NumArticles = as.numeric(NumArticles)
  ) |>
  group_by(ActionGeo_ADM1Code, year_week) |>
  summarise(
    # Retain country code for stratification
    ActionGeo_CountryCode = first(ActionGeo_CountryCode),
    
    # Target variable
    strike_count = n(),
    
    # Event characteristics
    avg_tone = mean(AvgTone, na.rm = TRUE),
    total_mentions = sum(NumMentions, na.rm = TRUE),
    total_articles = sum(NumArticles, na.rm = TRUE),
    
    # Actor diversity and involvement
    actor_diversity = n_distinct(paste(Actor1Type1Code, Actor2Type1Code), na.rm = TRUE),
    unique_actor1_types = n_distinct(Actor1Type1Code, na.rm = TRUE),
    unique_actor2_types = n_distinct(Actor2Type1Code, na.rm = TRUE),
    
    # Key actor involvement (binary flags)
    has_gov = any(str_detect(paste(Actor1Type1Code, Actor2Type1Code), "GOV"), na.rm = TRUE),
    has_labor = any(str_detect(paste(Actor1Type1Code, Actor2Type1Code), "LAB"), na.rm = TRUE),
    has_civil = any(str_detect(paste(Actor1Type1Code, Actor2Type1Code), "CVL"), na.rm = TRUE),
    
    # Proportional involvement
    prop_gov = mean(str_detect(paste(Actor1Type1Code, Actor2Type1Code), "GOV"), na.rm = TRUE),
    prop_labor = mean(str_detect(paste(Actor1Type1Code, Actor2Type1Code), "LAB"), na.rm = TRUE),
    prop_civil = mean(str_detect(paste(Actor1Type1Code, Actor2Type1Code), "CVL"), na.rm = TRUE),
    
    .groups = "drop"
  ) |>
  # Create date variable from year_week for temporal feature engineering
  mutate(
    year = year(year_week),
    week = week(year_week),
    date = year_week  # year_week is already the correct date
  ) |>
  arrange(ActionGeo_ADM1Code, date)

# Add lagged features and rolling averages
weekly_strikes <- weekly_strikes |>
  group_by(ActionGeo_ADM1Code) |>
  mutate(
    # Lagged strike counts
    strike_count_lag1 = lag(strike_count, 1),
    strike_count_lag2 = lag(strike_count, 2),
    strike_count_lag4 = lag(strike_count, 4),
    
    # Rolling averages
    rolling_avg_4wk = rollmean(strike_count, k = 4, fill = NA, align = "right"),
    rolling_avg_8wk = rollmean(strike_count, k = 8, fill = NA, align = "right"),
    
    # Lagged features for other variables
    avg_tone_lag1 = lag(avg_tone, 1),
    total_mentions_lag1 = lag(total_mentions, 1),
    actor_diversity_lag1 = lag(actor_diversity, 1),

    # Monotonic time trend
    time_trend = row_number(),
    
    # Cyclical features for longer-term patterns
    year_sin = sin(2 * pi * year(date) / 10), # 10-year cycle
    year_cos = cos(2 * pi * year(date) / 10)
  ) |>
  ungroup() |>
  # Add temporal indicators
  mutate(
    month = month(date),
    quarter = quarter(date),
    is_year_end = month %in% c(11, 12, 1)  # Holiday/year-end effect
  )

# Remove rows with insufficient lagged data for modeling
model_data <- weekly_strikes |>
  filter(!is.na(strike_count_lag2)) |>  # Ensure at least 2 weeks of history
  select(-year_week, -year, -week, -date)  # Remove redundant time variables

# Summary of processed data
cat("=== FINAL DATASET SUMMARY ===\n")
cat("Final dataset dimensions:", nrow(model_data), "x", ncol(model_data), "\n")
cat("Unique ADM1 regions:", n_distinct(model_data$ActionGeo_ADM1Code), "\n")
cat("Unique countries:", n_distinct(model_data$ActionGeo_CountryCode), "\n")
cat("Date range:", as.character(min(weekly_strikes$date, na.rm = TRUE)), "to", as.character(max(weekly_strikes$date, na.rm = TRUE)), "\n")
```

## Exploratory Data Analysis

### Descriptive Statistics and Visualizations

```{r}
#| label: eda
#| fig-width: 12
#| fig-height: 8

# Target variable distribution
p1 <- model_data |>
  ggplot(aes(x = strike_count)) +
  geom_histogram(bins = 50, alpha = 0.7) +
  scale_x_log10() +
  labs(title = "Distribution of Weekly Strike Counts (log scale)",
       x = "Strike Count", y = "Frequency")

# Time series of strikes
p2 <- weekly_strikes |>
  group_by(date) |>
  summarise(total_strikes = sum(strike_count), .groups = "drop") |>
  ggplot(aes(x = date, y = total_strikes)) +
  geom_line() +
  geom_smooth(method = "loess", se = FALSE) +
  labs(title = "Global Strike Activity Over Time",
       x = "Date", y = "Total Weekly Strikes")

# Top regions by strike activity
p3 <- model_data |>
  group_by(ActionGeo_ADM1Code) |>
  summarise(total_strikes = sum(strike_count), .groups = "drop") |>
  slice_max(total_strikes, n = 15) |>
  ggplot(aes(x = reorder(ActionGeo_ADM1Code, total_strikes), y = total_strikes)) +
  geom_col() +
  coord_flip() +
  labs(title = "Top 15 Regions by Total Strike Activity",
       x = "ADM1 Code", y = "Total Strikes")

# Temporal patterns
p4 <- model_data |>
  group_by(month) |>
  summarise(avg_strikes = mean(strike_count), .groups = "drop") |>
  ggplot(aes(x = month, y = avg_strikes)) +
  geom_col() +
  scale_x_continuous(breaks = 1:12, labels = month.abb) +
  labs(title = "Average Strike Activity by Month",
       x = "Month", y = "Average Strike Count")

grid.arrange(p1, p2, p3, p4, ncol = 2)
```

### Correlation Analysis

```{r}
#| label: comprehensive-correlation
#| fig-width: 14
#| fig-height: 10

# Comprehensive correlation matrix including all temporal features
cor_data_full <- model_data |>
  select(
    # Target and lagged targets
    strike_count, strike_count_lag1, strike_count_lag2, strike_count_lag4,
    
    # Rolling averages
    rolling_avg_4wk, rolling_avg_8wk,
    
    # Event characteristics and their lags
    avg_tone, avg_tone_lag1,
    total_mentions, total_mentions_lag1,
    total_articles,
    
    # Actor features
    actor_diversity, actor_diversity_lag1,
    unique_actor1_types, unique_actor2_types,
    prop_gov, prop_labor, prop_civil,
    
    # Temporal features
    time_trend, year_sin, year_cos,
    month, quarter
  ) |>
  # Convert logical to numeric for correlation
  mutate(across(everything(), as.numeric)) |>
  cor(use = "complete.obs")

# Create correlation heatmap with better labeling
corrplot(cor_data_full, 
         method = "color", 
         type = "upper",
         order = "hclust", 
         tl.cex = 0.7, 
         tl.col = "black",
         tl.srt = 45,
         title = "Comprehensive Feature Correlation Matrix",
         mar = c(0,0,2,0))
```

### Feature Importance and Predictive Power

```{r}
#| label: feature-importance-preview
#| fig-width: 12
#| fig-height: 6

# Quick feature correlation with target variable
target_correlations <- model_data |>
  select(-ActionGeo_ADM1Code, -ActionGeo_CountryCode) |>
  # Convert logical to numeric
  mutate(
    has_gov = as.numeric(has_gov),
    has_labor = as.numeric(has_labor), 
    has_civil = as.numeric(has_civil),
    is_year_end = as.numeric(is_year_end)
  ) |>
  cor(use = "complete.obs") |>
  as.data.frame() |>
  rownames_to_column("feature") |>
  select(feature, strike_count) |>
  filter(feature != "strike_count") |>
  arrange(desc(abs(strike_count))) |>
  slice_head(n = 20)

# Plot top correlations
p_corr <- target_correlations |>
  ggplot(aes(x = reorder(feature, abs(strike_count)), y = strike_count)) +
  geom_col(aes(fill = strike_count > 0)) +
  coord_flip() +
  scale_fill_manual(values = c("TRUE" = "steelblue", "FALSE" = "coral")) +
  labs(title = "Top 20 Features by Correlation with Strike Count",
       x = "Feature", y = "Correlation with Strike Count") +
  theme(legend.position = "none")

# Rolling averages effectiveness
p_rolling <- model_data |>
  select(strike_count, strike_count_lag1, rolling_avg_4wk, rolling_avg_8wk) |>
  pivot_longer(-strike_count, names_to = "predictor", values_to = "value") |>
  filter(!is.na(value)) |>
  ggplot(aes(x = value, y = strike_count)) +
  geom_point(alpha = 0.1) +
  geom_smooth(method = "lm", se = FALSE) +
  facet_wrap(~predictor, scales = "free_x") +
  scale_x_log10() +
  scale_y_log10() +
  labs(title = "Predictive Power of Lagged and Rolling Features",
       x = "Predictor Value (log scale)", y = "Strike Count (log scale)")

grid.arrange(p_corr, p_rolling, ncol = 2)
```

## Modeling with Tidymodels

### Model Setup and Temporal Split

```{r}
#| label: model-setup

# Temporal train/validation/test split (80/10/10) - essential for time series
# Avoids data leakage and follows realistic deployment scenario
set.seed(2024)

# Create model data with date preserved for proper temporal splitting
model_data_with_date <- weekly_strikes |>
  filter(!is.na(strike_count_lag2)) |>  # Same filter as model_data
  select(-year_week, -year, -week) |>  # Remove redundant time variables but keep date
  arrange(date) |>  # Ensure proper temporal ordering
  mutate(
    row_index = row_number(),
    total_rows = n(),
    # Calculate split points
    train_end = round(0.80 * total_rows),
    val_end = round(0.90 * total_rows)
  )

# Get split boundaries
split_info <- model_data_with_date |>
  slice(c(1, unique(train_end), unique(val_end), n())) |>
  summarise(
    train_start = first(date),
    train_end = nth(date, 2),
    val_start = nth(date, 2) + days(1),
    val_end = nth(date, 3),
    test_start = nth(date, 3) + days(1),
    test_end = last(date),
    .groups = "drop"
  )

cat("=== TEMPORAL SPLIT BOUNDARIES ===\n")
cat("Training:   ", as.character(split_info$train_start), "to", as.character(split_info$train_end), "\n")
cat("Validation: ", as.character(split_info$val_start), "to", as.character(split_info$val_end), "\n")
cat("Test:       ", as.character(split_info$test_start), "to", as.character(split_info$test_end), "\n\n")

# Create the splits based on temporal ordering
train_data <- model_data_with_date |>
  filter(row_index <= unique(train_end)) |>
  select(-date, -row_index, -total_rows, -train_end, -val_end)  # Remove helper columns

val_data <- model_data_with_date |>
  filter(row_index > unique(train_end) & row_index <= unique(val_end)) |>
  select(-date, -row_index, -total_rows, -train_end, -val_end)

test_data <- model_data_with_date |>
  filter(row_index > unique(val_end)) |>
  select(-date, -row_index, -total_rows, -train_end, -val_end)

# Create indices for tidymodels compatibility
train_indices <- which(model_data_with_date$row_index <= unique(model_data_with_date$train_end))
val_indices <- which(model_data_with_date$row_index > unique(model_data_with_date$train_end) & 
                     model_data_with_date$row_index <= unique(model_data_with_date$val_end))
test_indices <- which(model_data_with_date$row_index > unique(model_data_with_date$val_end))

# Create validation split object for tidymodels (train vs validation)
val_split <- list(
  train = train_indices,
  val = val_indices
)

cat("Training set:   ", nrow(train_data), "observations (", round(100*nrow(train_data)/nrow(model_data_with_date), 1), "%)\n")
cat("Validation set: ", nrow(val_data), "observations (", round(100*nrow(val_data)/nrow(model_data_with_date), 1), "%)\n")
cat("Test set:       ", nrow(test_data), "observations (", round(100*nrow(test_data)/nrow(model_data_with_date), 1), "%)\n\n")

cat("Country representation:\n")
cat("Training:   ", n_distinct(train_data$ActionGeo_CountryCode), "countries\n")
cat("Validation: ", n_distinct(val_data$ActionGeo_CountryCode), "countries\n") 
cat("Test:       ", n_distinct(test_data$ActionGeo_CountryCode), "countries\n")
```

### Recipe Preprocessing

```{r}
#| label: recipes

# Recipe for tree-based models (XGBoost, LightGBM)
# Use dummy encoding since these engines require numeric inputs
tree_recipe <- recipe(strike_count ~ ., data = train_data) |>
  # Handle categorical variables
  step_novel(ActionGeo_ADM1Code, ActionGeo_CountryCode) |>  
  step_other(ActionGeo_ADM1Code, threshold = 0.01) |>  # Group rare ADM1s
  step_dummy(ActionGeo_ADM1Code, ActionGeo_CountryCode) |>  # Convert to dummy variables
  # Convert logical to numeric
  step_mutate(
    has_gov = as.numeric(has_gov),
    has_labor = as.numeric(has_labor),
    has_civil = as.numeric(has_civil),
    is_year_end = as.numeric(is_year_end)
  ) |>
  step_dummy(all_nominal_predictors()) |>  # Handle any remaining categorical variables
  step_impute_median(all_numeric_predictors()) |>
  step_zv(all_predictors())

# Recipe for linear models (Random Forest, Linear Regression)  
# Need dummy encoding for these algorithms
linear_recipe <- recipe(strike_count ~ ., data = train_data) |>
  # Handle categorical variables
  step_novel(ActionGeo_ADM1Code, ActionGeo_CountryCode) |>
  step_other(ActionGeo_ADM1Code, threshold = 0.01) |>
  step_dummy(ActionGeo_ADM1Code, ActionGeo_CountryCode) |>
  # Convert logical to numeric
  step_mutate(
    has_gov = as.numeric(has_gov),
    has_labor = as.numeric(has_labor), 
    has_civil = as.numeric(has_civil),
    is_year_end = as.numeric(is_year_end)
  ) |>
  step_dummy(all_nominal_predictors()) |>
  step_impute_median(all_numeric_predictors()) |>
  step_zv(all_predictors()) |>  # Remove zero-variance columns BEFORE normalization
  step_normalize(all_numeric_predictors())

cat("Created two recipes:\n")
cat("1. tree_recipe: For XGBoost and LightGBM (keeps categorical variables)\n")
cat("2. linear_recipe: For Random Forest and Linear Regression (dummy encoding + normalization)\n")
```

### Model Specifications and Workflows

```{r}
#| label: model-specs

library(bonsai)

# XGBoost model specification
xgb_spec <- boost_tree(
  trees = tune(),
  tree_depth = tune(),
  min_n = tune(),
  loss_reduction = tune(),
  sample_size = tune(),
  mtry = tune(),
  learn_rate = tune()
) |>
  set_engine("xgboost") |>
  set_mode("regression")

# LightGBM model specification
lgb_spec <- boost_tree(
  trees = tune(),
  tree_depth = tune(),
  min_n = tune(),
  loss_reduction = tune(),
  sample_size = tune(),
  mtry = tune(),
  learn_rate = tune()
) |>
  set_engine("lightgbm") |>
  set_mode("regression")

# Random Forest model specification  
rf_spec <- rand_forest(
  trees = 1000,
  mtry = tune(),
  min_n = tune()
) |>
  set_engine("ranger") |>
  set_mode("regression")

# Linear regression baseline
lm_spec <- linear_reg() |>
  set_engine("lm") |>
  set_mode("regression")
```

### Workflows for Model Training

```{r}
#| label: workflows

# Create workflows with appropriate recipes
xgb_wf <- workflow() |>
  add_recipe(tree_recipe) |>  # Use tree recipe (keeps categorical variables)
  add_model(xgb_spec)

lgb_wf <- workflow() |>
  add_recipe(tree_recipe) |>  # Use tree recipe
  add_model(lgb_spec)

rf_wf <- workflow() |>
  add_recipe(linear_recipe) |>  # Use linear recipe (dummy encoding + normalization)
  add_model(rf_spec)

lm_wf <- workflow() |>
  add_recipe(linear_recipe) |>  # Use linear recipe
  add_model(lm_spec)
```

### Model Tuning and Validation

```{r}
#| label: tuning
#| cache: true
#| message: false

# Create validation split object for tidymodels tuning
# Combine train and validation data for the resampling object
train_val_data <- bind_rows(
  train_data |> mutate(.row = train_indices),
  val_data |> mutate(.row = val_indices)
)

# Create validation split for tune_grid
val_set <- make_splits(
  list(analysis = train_indices, assessment = val_indices),
  train_val_data
)
val_resamples <- manual_rset(list(val_set), c("validation"))

# Tune XGBoost (reduced grid for speed)
xgb_grid <- grid_space_filling(
  trees(range = c(100, 1000)),
  tree_depth(range = c(3, 8)),
  min_n(range = c(2, 20)),
  loss_reduction(range = c(-10, 1.5)),
  sample_size = sample_prop(range = c(0.5, 1.0)),
  mtry(range = c(5, 15)),
  learn_rate(range = c(-3, -1)),
  size = 20  # Small grid for preliminary analysis
)

# Tune XGBoost on validation set
xgb_tune <- tune_grid(
  xgb_wf,
  resamples = val_resamples,
  grid = xgb_grid,
  metrics = metric_set(rmse, rsq, mae),
  control = control_grid(save_pred = FALSE, verbose = FALSE)
)

# Tune LightGBM (using same grid as XGBoost)
lgb_tune <- tune_grid(
  lgb_wf,
  resamples = val_resamples,
  grid = xgb_grid,  # Can reuse the same grid
  metrics = metric_set(rmse, rsq, mae),
  control = control_grid(save_pred = FALSE, verbose = FALSE)
)

# Tune Random Forest
rf_grid <- grid_regular(
  mtry(range = c(5, 15)),
  min_n(range = c(2, 20)),
  levels = 4  # Small grid
)

rf_tune <- tune_grid(
  rf_wf,
  resamples = val_resamples,
  grid = rf_grid,
  metrics = metric_set(rmse, rsq, mae),
  control = control_grid(save_pred = FALSE, verbose = FALSE)
)

# Fit linear regression baseline on validation set
lm_fit <- fit_resamples(
  lm_wf,
  resamples = val_resamples,
  metrics = metric_set(rmse, rsq, mae),
  control = control_resamples(save_pred = FALSE)
)
```

### Model Evaluation

```{r}
#| label: model-comparison

# Collect validation results
xgb_results <- collect_metrics(xgb_tune) |>
  mutate(model = "XGBoost")

lgb_results <- collect_metrics(lgb_tune) |>
  mutate(model = "LightGBM")

rf_results <- collect_metrics(rf_tune) |>
  mutate(model = "Random Forest")

lm_results <- collect_metrics(lm_fit) |>
  mutate(model = "Linear Regression")

# Compare best models based on validation RMSE
best_results <- bind_rows(
  xgb_results |> filter(.metric == "rmse") |> slice_min(mean) |> select(model, .metric, mean, std_err),
  lgb_results |> filter(.metric == "rmse") |> slice_min(mean) |> select(model, .metric, mean, std_err),
  rf_results |> filter(.metric == "rmse") |> slice_min(mean) |> select(model, .metric, mean, std_err),
  lm_results |> filter(.metric == "rmse") |> select(model, .metric, mean, std_err)
) |>
  arrange(mean)

print("Validation set RMSE comparison:")
print(best_results)

# Get best model based on validation performance
best_model_name <- best_results$model[1]
if (best_model_name == "XGBoost") {
  best_params <- select_best(xgb_tune, metric = "rmse")
  final_wf <- finalize_workflow(xgb_wf, best_params)
} else if (best_model_name == "LightGBM") {
  best_params <- select_best(lgb_tune, metric = "rmse")
  final_wf <- finalize_workflow(lgb_wf, best_params)
} else if (best_model_name == "Random Forest") {
  best_params <- select_best(rf_tune, metric = "rmse")
  final_wf <- finalize_workflow(rf_wf, best_params)
} else {
  # Linear regression has no hyperparameters to tune
  final_wf <- lm_wf
}

cat("Best model selected:", best_model_name, "\n")
cat("Best validation RMSE:", round(best_results$mean[1], 3), "\n")
```

### Final Model Fit and Evaluation

```{r}
#| label: final-model

# Fit final model on combined train + validation data
train_val_combined <- bind_rows(train_data, val_data)
final_fit <- fit(final_wf, data = train_val_combined)

# Make predictions on test set
test_predictions <- predict(final_fit, new_data = test_data) |>
  bind_cols(test_data |> select(strike_count))

# Calculate test set metrics
test_metrics <- test_predictions |>
  metrics(truth = strike_count, estimate = .pred)

print("Test set performance:")
print(test_metrics)

# Variable importance
vip(final_fit, num_features = 15)

cat("\n=== FINAL MODEL SUMMARY ===\n")
cat("Model trained on:", nrow(train_val_combined), "observations (train + validation)\n")
cat("Model tested on:", nrow(test_data), "observations\n")
cat("Final test RMSE:", round(test_metrics$.estimate[test_metrics$.metric == "rmse"], 3), "\n")
cat("Final test R²:", round(test_metrics$.estimate[test_metrics$.metric == "rsq"], 3), "\n")
```

## Summary

```{r}
#| label: summary

# Model performance summary
cat("=== PRELIMINARY STRIKE PREDICTION RESULTS ===\n\n")
cat("Dataset: GDELT strikes (2015-present)\n")
cat("Observations:", nrow(model_data), "weekly aggregations\n")
cat("Regions:", n_distinct(model_data$ActionGeo_ADM1Code), "ADM1 codes\n")
cat("Countries:", n_distinct(model_data$ActionGeo_CountryCode), "countries\n\n")

cat("Methodology:\n")
cat("- Temporal train/validation/test split (80/10/10)\n")
cat("- Training:", as.character(split_info$train_start), "to", as.character(split_info$train_end), "\n")
cat("- Validation:", as.character(split_info$val_start), "to", as.character(split_info$val_end), "\n") 
cat("- Test:", as.character(split_info$test_start), "to", as.character(split_info$test_end), "\n")
cat("- Prevents data leakage and follows realistic prediction scenario\n\n")

cat("Best model:", best_model_name, "\n")
cat("Validation RMSE:", round(best_results$mean[1], 3), "\n")
cat("Test RMSE:", round(test_metrics$.estimate[test_metrics$.metric == "rmse"], 3), "\n")
cat("Test R²:", round(test_metrics$.estimate[test_metrics$.metric == "rsq"], 3), "\n\n")

cat("Key findings for abstract:\n")
cat("- Temporal features (lagged counts, rolling averages) are most predictive\n")
cat("- Media attention (mentions, articles) provides signal\n") 
cat("- Actor diversity and involvement patterns matter\n")
cat("- GDELT sentiment (tone) adds predictive value\n")
cat("- Time series approach with proper temporal split essential for realistic evaluation\n")
```

## Basic Diagnostic plots

```{r}
library(patchwork)

# Predictions vs Actual
p1 <- ggplot(test_predictions, aes(x = strike_count, y = .pred)) +
  geom_point(alpha = 0.3) +
  geom_abline(slope = 1, intercept = 0, color = "red") +
  labs(title = "Predictions vs Actual", x = "Actual Strike Count", y = "Predicted")

# Residuals vs Fitted
p2 <- test_predictions |>
  mutate(residuals = strike_count - .pred) |>
  ggplot(aes(x = .pred, y = residuals)) +
  geom_point(alpha = 0.3) +
  geom_hline(yintercept = 0, color = "red") +
  labs(title = "Residuals vs Fitted", x = "Fitted Values", y = "Residuals")

diagnostic_plots <- p1 + p2 + 
  plot_layout(ncol = 2) +
  plot_annotation(title = "Model Diagnostics")

diagnostic_plots
```

## Temporal Analysis

```{r}
# Prep data
test_with_dates <- model_data_with_date |>
  dplyr::slice(test_indices) |> 
  select(date, ActionGeo_ADM1Code, strike_count) |>
  bind_cols(test_predictions |> select(.pred))

# Then create temporal plots
p3 <- test_with_dates |>
  mutate(residuals = strike_count - .pred) |>
  ggplot(aes(x = date, y = residuals)) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = "loess") +
  labs(title = "Residuals Over Time")

# Performance over time (in addition to your residuals plot)
p4 <- test_with_dates |>
  ggplot(aes(x = date)) +
  geom_line(aes(y = strike_count, color = "Actual"), alpha = 0.7) +
  geom_line(aes(y = .pred, color = "Predicted"), alpha = 0.7) +
  labs(title = "Actual vs Predicted Over Time", y = "Strike Count") +
  theme(legend.title = element_blank())

# Monthly performance stability  
p5 <- test_with_dates |>
  mutate(month = floor_date(date, "month")) |>
  group_by(month) |>
  summarise(rmse = sqrt(mean((strike_count - .pred)^2)), .groups = "drop") |>
  ggplot(aes(x = month, y = rmse)) +
  geom_line() +
  geom_point() +
  labs(title = "Model RMSE by Month", y = "RMSE") 

temporal_analysis <- p3 / p4 / p5 + 
  plot_annotation(title = "Temporal Analysis of Model Performance")

temporal_analysis
```

## Future Analysis Enhancements

```{r}
#| label: future-suggestions
#| eval: false

# ========================================
# SUGGESTIONS FOR FUTURE ITERATIONS
# ========================================

cat("=== ENHANCEMENTS FOR DEEPER ANALYSIS ===\n\n")

# 1. SPATIAL FEATURES
cat("1. SPATIAL ENHANCEMENTS:\n")
cat("- Add spatial lag variables (strikes in neighboring ADM1s)\n")
cat("- Include distance-weighted spillover effects\n")
cat("- Add country-level economic/political indicators\n")
cat("- Consider geographic clustering (regions with similar patterns)\n\n")

# Example spatial lag calculation:
# spatial_lags <- weekly_strikes %>%
#   left_join(adm1_neighbors, by = "ActionGeo_ADM1Code") %>%
#   group_by(ActionGeo_ADM1Code, date) %>%
#   summarise(neighbor_strikes = sum(neighbor_strike_count, na.rm = TRUE))

# 2. ADVANCED TEMPORAL FEATURES  
cat("2. TEMPORAL SOPHISTICATION:\n")
cat("- Seasonal decomposition and detrending\n")
cat("- Holiday/event calendars by country\n")
cat("- Autoregressive error terms\n")
cat("- Time-varying coefficients\n\n")

# 3. CLASS IMBALANCE HANDLING
cat("3. CLASS IMBALANCE (if many zero strike weeks):\n")
cat("- Two-stage modeling: occurrence then intensity\n")
cat("- Hurdle/zero-inflated models\n")
cat("- SMOTE or other resampling techniques\n")
cat("- Cost-sensitive learning\n\n")

# 4. MODEL INTERPRETATION
cat("4. ENHANCED INTERPRETATION:\n")
cat("- SHAP values for individual predictions\n")
cat("- Partial dependence plots\n")
cat("- Feature interaction analysis\n")
cat("- Time-varying importance\n\n")

# Example SHAP analysis:
# library(shapr)
# shap_values <- explain(final_fit, x_test = test_data[1:100, ])

# 5. ENSEMBLE METHODS
cat("5. ENSEMBLE APPROACHES:\n")
cat("- Stacking different model types\n")
cat("- Time series specific ensembles\n")
cat("- Bayesian model averaging\n")
cat("- Dynamic ensemble weights\n\n")

# 6. VALIDATION SOPHISTICATION
cat("6. ADVANCED VALIDATION:\n")
cat("- Time series cross-validation (expanding window)\n")
cat("- Nested CV for hyperparameter stability\n")
cat("- Forecast evaluation metrics (MAPE, sMAPE)\n")
cat("- Prediction intervals and uncertainty quantification\n\n")

# 7. EXTERNAL DATA INTEGRATION
cat("7. EXTERNAL DATA SOURCES:\n")
cat("- Economic indicators (GDP, unemployment, inflation)\n")
cat("- Political events and elections\n")
cat("- Weather/climate variables\n")
cat("- Social media sentiment\n")
cat("- News volume and topic modeling\n\n")

# 8. SCALE AND EFFICIENCY
cat("8. COMPUTATIONAL IMPROVEMENTS:\n")
cat("- Larger hyperparameter grids\n")
cat("- Parallel processing for CV\n")
cat("- Early stopping for efficiency\n")
cat("- Model compression for deployment\n\n")

cat("=== IMMEDIATE NEXT STEPS ===\n")
cat("For your next iteration, prioritize:\n")
cat("1. Run current analysis and examine variable importance\n")
cat("2. Add spatial lag features (neighboring ADM1 effects)\n") 
cat("3. Implement time series CV for more robust validation\n")
cat("4. Add SHAP analysis for model interpretation\n")
cat("5. Consider zero-inflation if many zero-strike weeks\n")
```

## Save Model Objects and Diagnostics

```{r}
#| label: save-objects

# 1. Save key model performance objects
model_results <- list(
  # Model object
  final_model = final_fit,
  best_model_name = best_model_name,
  
  # Performance metrics
  test_metrics = test_metrics,
  best_validation_results = best_results,
  
  # Data used for evaluation
  test_predictions = test_predictions,
  test_with_dates = test_with_dates,
  
  # Split information
  split_boundaries = split_info,
  
  # Model comparison results
  xgb_results = xgb_results,
  lgb_results = lgb_results, 
  rf_results = rf_results,
  lm_results = lm_results
)

# Save R objects
saveRDS(model_results, "model_outputs/strike_prediction_results.rds")

# 2. Create and save variable importance plot
vip_plot <- vip(final_fit, num_features = 15) +
  labs(title = "Variable Importance - Strike Prediction Model")

# 3. Save all plots as high-quality images
ggsave("model_outputs/variable_importance.png", vip_plot, 
       width = 10, height = 6, dpi = 300)

ggsave("model_outputs/diagnostic_plots.png", diagnostic_plots,
       width = 12, height = 6, dpi = 300)

ggsave("model_outputs/temporal_analysis.png", temporal_analysis,
       width = 10, height = 12, dpi = 300)

# 4. Save individual plots for flexibility
ggsave("model_outputs/predictions_vs_actual.png", p1, 
       width = 6, height = 6, dpi = 300)
ggsave("model_outputs/residuals_vs_fitted.png", p2,
       width = 6, height = 6, dpi = 300)
ggsave("model_outputs/residuals_over_time.png", p3,
       width = 10, height = 4, dpi = 300)
ggsave("model_outputs/actual_vs_predicted_time.png", p4,
       width = 10, height = 4, dpi = 300)
ggsave("model_outputs/rmse_by_month.png", p5,
       width = 10, height = 4, dpi = 300)

# 5. Create summary CSV files
# Model performance summary
performance_summary <- data.frame(
  model = best_model_name,
  validation_rmse = round(best_results$mean[1], 3),
  test_rmse = round(test_metrics$.estimate[test_metrics$.metric == "rmse"], 3),
  test_rsq = round(test_metrics$.estimate[test_metrics$.metric == "rsq"], 3),
  test_mae = round(test_metrics$.estimate[test_metrics$.metric == "mae"], 3),
  training_obs = nrow(train_val_combined),
  test_obs = nrow(test_data)
)

write.csv(performance_summary, "model_outputs/model_performance_summary.csv", row.names = FALSE)

# Model comparison results
all_model_results <- bind_rows(
  xgb_results |> filter(.metric == "rmse") |> slice_min(mean),
  lgb_results |> filter(.metric == "rmse") |> slice_min(mean), 
  rf_results |> filter(.metric == "rmse") |> slice_min(mean),
  lm_results |> filter(.metric == "rmse")
) |>
  select(model, .metric, mean, std_err) |>
  arrange(mean)

write.csv(all_model_results, "model_outputs/model_comparison.csv", row.names = FALSE)

cat("=== FILES SAVED ===\n")
cat("R objects: model_outputs/strike_prediction_results.rds\n")
cat("Plots saved as PNG files in model_outputs/\n")
cat("- variable_importance.png\n")
cat("- diagnostic_plots.png (combined)\n") 
cat("- temporal_analysis.png (combined)\n")
cat("- Individual plot files\n")
cat("CSV summaries:\n")
cat("- model_performance_summary.csv\n")
cat("- model_comparison.csv\n")
```